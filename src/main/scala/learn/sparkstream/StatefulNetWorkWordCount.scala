package learn.sparkstream

import org.apache.log4j.{Level, Logger}
import org.apache.spark.{HashPartitioner, SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  *  这个类是用来学习 updateStateByKey 函数的， 记录每个单词的统计量
  *  不是像之前一样每次都是只统计了一行，
  */
object StatefulNetWorkWordCount {

	def main(args: Array[String]): Unit = {
		if (args.length < 2) {
			System.err.println("Usage: StatefulNetworkWordCount <hostname> <port>")
			System.exit(1)
		}

		Logger.getRootLogger.setLevel(Level.WARN)
		val updateFunc = (values: Seq[Int], state: Option[Int]) => {
			val currentCount = values.sum
			val previousCount = state.getOrElse(0)
			Some(currentCount + previousCount)
		}

		val newUpdateFunc = (iterator: Iterator[(String, Seq[Int], Option[Int])]) => {
			iterator.flatMap(t => updateFunc(t._2, t._3).map(s => (t._1, s)))
		}

		val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCount").setMaster("local[3]")
		// Create the context with a 1 second batch size
		val ssc = new StreamingContext(sparkConf, Seconds(1))
		ssc.checkpoint(".")

		// Initial RDD input to updateStateByKey
		val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))
		// Create a ReceiverInputDStream on target ip:port and count the
		// words in input stream of \n delimited test (eg. generated by 'nc')
		val lines = ssc.socketTextStream(args(0), args(1).toInt)

		val word = lines.flatMap(x => x.trim.split(" "))

		val wordStream = word.map(x => (x, 1))


		// Update the cumulative count using updateStateByKey
		// This will give a Dstream made of state (which is the cumulative count of the words)
		val stateDstream = wordStream.updateStateByKey[Int](newUpdateFunc,
			new HashPartitioner (ssc.sparkContext.defaultParallelism), true, initialRDD)
		stateDstream.print()
		ssc.start()
		ssc.awaitTermination()

	}


}
